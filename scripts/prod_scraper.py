#!/usr/bin/env python3
"""
Production Scraper Entry Point
Iterates through all companies in the database and runs the appropriate scraper.
"""
import os
import sys
import asyncio
import logging
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.database.connection import get_db, close_db
from src.scrapers.greenhouse import GreenhouseScraper
from src.scrapers.lever import LeverScraper
from src.scrapers.workday import WorkdayScraper
from src.scrapers.ashby import AshbyScraper

# Ensure logs directory exists BEFORE logging configuration
Path("logs").mkdir(exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/scraper_prod.log', encoding='utf-8')
    ]
)
logger = logging.getLogger("ProdScraper")

async def scrape_company(company, scrapers, db, semaphore):
    """æŠ“å–å•ä¸ªå…¬å¸çš„èŒä½"""
    async with semaphore:
        # Determine ATS type
        ats_type = None
        ats_url = company.get('ats_url')
        
        # 1. Try to detect from ATS URL
        if ats_url:
            if 'greenhouse.io' in ats_url:
                ats_type = 'greenhouse'
            elif 'lever.co' in ats_url:
                ats_type = 'lever'
            elif 'workday.com' in ats_url or 'myworkdayjobs.com' in ats_url:
                ats_type = 'workday'
            elif 'ashbyhq.com' in ats_url:
                ats_type = 'ashby'
            elif 'workable.com' in ats_url:
                ats_type = 'workable'
                
        # 2. Fallback to configured type
        if not ats_type:
            ats_info = company.get('ats_system', {})
            if isinstance(ats_info, dict):
                ats_type = ats_info.get('type')
        
        if not ats_type:
            logger.warning(f"è·³è¿‡ {company['name']}: æœªå®šä¹‰ ATS ç±»å‹ä¸”æ— æ³•ä» URL è¯†åˆ«")
            return 0
            
        scraper = scrapers.get(ats_type.lower())
        if not scraper:
            logger.warning(f"è·³è¿‡ {company['name']}: å°šæœªå®ç° '{ats_type}' çš„æŠ“å–å™¨")
            return 0
            
        try:
            logger.info(f"æ­£åœ¨æŠ“å– {company['name']} (ä½¿ç”¨ {ats_type})...")
            jobs = await scraper.scrape(company)
            
            if jobs:
                # Save/Update jobs in DB
                saved_count = 0
                for job in jobs:
                    # 1. æŸ¥æ‰¾ç°æœ‰èŒä½
                    existing_job = db.jobs.find_one({'job_id': job['job_id']})
                    
                    if existing_job:
                        # 2. å¦‚æœå­˜åœ¨ï¼Œæ£€æŸ¥å“ˆå¸Œå€¼æ˜¯å¦ä¸€è‡´
                        if existing_job.get('content_hash') == job.get('content_hash'):
                            # å“ˆå¸Œä¸€è‡´ï¼Œåªæ›´æ–° last_seen_at
                            db.jobs.update_one(
                                {'job_id': job['job_id']},
                                {'$set': {'last_seen_at': job['last_seen_at'], 'is_active': True}}
                            )
                        else:
                            # å“ˆå¸Œä¸ä¸€è‡´ï¼Œå…¨é‡æ›´æ–°
                            db.jobs.update_one(
                                {'job_id': job['job_id']},
                                {'$set': job},
                                upsert=True
                            )
                            saved_count += 1
                    else:
                        # 3. å¦‚æœä¸å­˜åœ¨ï¼Œç›´æ¥æ’å…¥
                        db.jobs.insert_one(job)
                        saved_count += 1
                
                logger.info(f"âœ… {company['name']}: å¤„ç†å®Œæˆã€‚æ–°å¢/æ›´æ–°: {saved_count}ï¼Œè·³è¿‡: {len(jobs) - saved_count}")
                return saved_count
            else:
                logger.info(f"â„¹ï¸ {company['name']}: æœªæ‰¾åˆ°èŒä½")
                return 0
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å– {company['name']} å¤±è´¥: {e}")
            return 0

async def run_production_scrape():
    db = get_db()
    
    # Initialize scrapers
    scrapers = {
        'greenhouse': GreenhouseScraper(),
        'lever': LeverScraper(),
        'workday': WorkdayScraper(),
        'ashby': AshbyScraper(),
    }
    
    # 1. Fetch all companies from DB
    companies = list(db.companies.find({}))
    if not companies:
        logger.warning("æ•°æ®åº“ä¸­æ²¡æœ‰å…¬å¸ä¿¡æ¯ã€‚è¯·å…ˆè¿è¡Œ scripts/import_companies.py")
        return

    logger.info(f"ğŸš€ å¼€å§‹ä¸º {len(companies)} å®¶å…¬å¸è¿›è¡Œç”Ÿäº§æŠ“å–...")
    
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¢«å°ç¦
    semaphore = asyncio.Semaphore(5)
    
    # åˆ›å»ºæ‰€æœ‰æŠ“å–ä»»åŠ¡
    tasks = [scrape_company(company, scrapers, db, semaphore) for company in companies]
    
    # å¹¶è¡Œè¿è¡Œ
    results = await asyncio.gather(*tasks)
    
    total_new_jobs = sum(results)

    logger.info(f"ğŸ ç”Ÿäº§æŠ“å–å®Œæˆã€‚æ€»è®¡æ›´æ–°/æ–°å¢: {total_new_jobs} ä¸ªèŒä½")
    close_db()

if __name__ == "__main__":
    asyncio.run(run_production_scrape())
