#!/usr/bin/env python3
"""
Production Scraper Entry Point
Iterates through all companies in the database and runs the appropriate scraper.
"""
import os
import sys
import asyncio
import logging
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.database.connection import get_db, close_db
from src.scrapers.greenhouse import GreenhouseScraper
from src.scrapers.lever import LeverScraper
from src.scrapers.workday import WorkdayScraper
from src.scrapers.ashby import AshbyScraper
from src.scrapers.workable import WorkableScraper
from src.services.language_filter import LanguageFilterService

# Ensure logs directory exists BEFORE logging configuration
Path("logs").mkdir(exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/scraper_prod.log', encoding='utf-8')
    ]
)
logger = logging.getLogger("ProdScraper")

async def scrape_company(company, scrapers, db, semaphore):
    """æŠ“å–å•ä¸ªå…¬å¸çš„èŒä½"""
    async with semaphore:
        # Determine ATS type
        ats_type = None
        ats_url = company.get('ats_url')
        
        # 1. Try to detect from ATS URL
        if ats_url:
            if 'greenhouse.io' in ats_url:
                ats_type = 'greenhouse'
            elif 'lever.co' in ats_url:
                ats_type = 'lever'
            elif 'workday.com' in ats_url or 'myworkdayjobs.com' in ats_url:
                ats_type = 'workday'
            elif 'ashbyhq.com' in ats_url:
                ats_type = 'ashby'
            elif 'workable.com' in ats_url:
                ats_type = 'workable'
                
        # 2. Fallback to configured type
        if not ats_type:
            ats_info = company.get('ats_system', {})
            if isinstance(ats_info, dict):
                ats_type = ats_info.get('type')
        
        if not ats_type:
            logger.warning(f"è·³è¿‡ {company['name']}: æœªå®šä¹‰ ATS ç±»å‹ä¸”æ— æ³•ä» URL è¯†åˆ«")
            return 0
            
        scraper = scrapers.get(ats_type.lower())
        if not scraper:
            logger.warning(f"è·³è¿‡ {company['name']}: å°šæœªå®ç° '{ats_type}' çš„æŠ“å–å™¨")
            return 0
            
        try:
            logger.info(f"æ­£åœ¨æŠ“å– {company['name']} (ä½¿ç”¨ {ats_type})...")
            jobs = await scraper.scrape(company)
            
            if jobs:
                # Save/Update jobs in DB
                saved_count = 0
                for job in jobs:
                    # 0. Content analysis (English-Only & IT-Only check)
                    job_text = job.get('description', '') + ' ' + job.get('title', '')
                    
                    # A. Category Check (IT Only)
                    is_it, it_reason = LanguageFilterService.is_it_role(job['title'], is_title=True) # Prioritize title for category
                    if not is_it:
                        # Fallback to description if title is ambiguous
                        is_it_desc, it_reason_desc = LanguageFilterService.is_it_role(job_text, is_title=False)
                        if not is_it_desc:
                            logger.info(f"ğŸš« {company['name']}: èŒä½ '{job['title']}' é IT èŒä½è¢«è¿‡æ»¤: {it_reason_desc}")
                            db.rejected_jobs.update_one(
                                {'content_hash': job['content_hash']},
                                {'$set': {'title': job['title'], 'company': job['company'], 'rejected_at': datetime.utcnow(), 'reason': f"Non-IT: {it_reason_desc}"}},
                                upsert=True
                            )
                            continue

                    # B. Language Check (English Only)
                    is_eng, eng_reason = LanguageFilterService.is_english_only(job_text)
                    if not is_eng:
                        logger.info(f"ğŸš« {company['name']}: èŒä½ '{job['title']}' å› è¦æ±‚æ—¥è¯­è¢«è¿‡æ»¤: {eng_reason}")
                        db.rejected_jobs.update_one(
                            {'content_hash': job['content_hash']},
                            {'$set': {'title': job['title'], 'company': job['company'], 'rejected_at': datetime.utcnow(), 'reason': f"Language: {eng_reason}"}},
                            upsert=True
                        )
                        continue

                    # 0.5 Check if already rejected
                    if db.rejected_jobs.find_one({'content_hash': job['content_hash']}):
                        logger.debug(f"â­ï¸ {company['name']}: è·³è¿‡å·²çŸ¥ä¸ç¬¦åˆè¦æ±‚çš„èŒä½ '{job['title']}'")
                        continue

                    # 1. æŸ¥æ‰¾ç°æœ‰èŒä½
                    existing_job = db.jobs.find_one({'job_id': job['job_id']})
                    
                    if existing_job:
                        # 2. å¦‚æœå­˜åœ¨ï¼Œæ£€æŸ¥å“ˆå¸Œå€¼æ˜¯å¦ä¸€è‡´
                        if existing_job.get('content_hash') == job.get('content_hash'):
                            # å“ˆå¸Œä¸€è‡´ï¼Œåªæ›´æ–° last_seen_at
                            db.jobs.update_one(
                                {'job_id': job['job_id']},
                                {'$set': {'last_seen_at': job['last_seen_at'], 'is_active': True}}
                            )
                        else:
                            # å“ˆå¸Œä¸ä¸€è‡´ï¼Œå…¨é‡æ›´æ–°
                            db.jobs.update_one(
                                {'job_id': job['job_id']},
                                {'$set': job},
                                upsert=True
                            )
                            saved_count += 1
                    else:
                        # 3. å¦‚æœä¸å­˜åœ¨ï¼Œç›´æ¥æ’å…¥
                        db.jobs.insert_one(job)
                        saved_count += 1
                
                logger.info(f"âœ… {company['name']}: å¤„ç†å®Œæˆã€‚æ–°å¢/æ›´æ–°: {saved_count}ï¼Œè·³è¿‡: {len(jobs) - saved_count}")
                return saved_count
            else:
                logger.info(f"â„¹ï¸ {company['name']}: æœªæ‰¾åˆ°èŒä½")
                return 0
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å– {company['name']} å¤±è´¥: {e}")
            return 0

async def run_production_scrape():
    db = get_db()
    
    # Initialize scrapers
    scrapers = {
        'greenhouse': GreenhouseScraper(),
        'lever': LeverScraper(),
        'workday': WorkdayScraper(),
        'ashby': AshbyScraper(),
        'workable': WorkableScraper(),
    }
    
    # 1. Fetch all companies from DB
    companies = list(db.companies.find({}))
    if not companies:
        logger.warning("æ•°æ®åº“ä¸­æ²¡æœ‰å…¬å¸ä¿¡æ¯ã€‚è¯·å…ˆè¿è¡Œ scripts/import_companies.py")
        return

    logger.info(f"ğŸš€ å¼€å§‹ä¸º {len(companies)} å®¶å…¬å¸è¿›è¡Œç”Ÿäº§æŠ“å–...")
    
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¢«å°ç¦
    semaphore = asyncio.Semaphore(5)
    
    # åˆ›å»ºæ‰€æœ‰æŠ“å–ä»»åŠ¡
    tasks = [scrape_company(company, scrapers, db, semaphore) for company in companies]
    
    # å¹¶è¡Œè¿è¡Œ
    results = await asyncio.gather(*tasks)
    
    total_new_jobs = sum(results)

    logger.info(f"ğŸ ç”Ÿäº§æŠ“å–å®Œæˆã€‚æ€»è®¡æ›´æ–°/æ–°å¢: {total_new_jobs} ä¸ªèŒä½")
    close_db()

if __name__ == "__main__":
    asyncio.run(run_production_scrape())
