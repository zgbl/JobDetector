#!/usr/bin/env python3
"""
Test Greenhouse Scraper
æµ‹è¯•Greenhouseé‡‡é›†å™¨ï¼ŒæŠ“å–å‡ å®¶å…¬å¸çš„èŒä½
"""
import sys
import asyncio
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scrapers.greenhouse import GreenhouseScraper
from src.database.connection import get_db, close_db
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_single_company(scraper, company_name: str, domain: str):
    """æµ‹è¯•å•ä¸ªå…¬å¸"""
    company = {
        'name': company_name,
        'domain': domain,
        'ats_system': {'type': 'greenhouse'}
    }
    
    logger.info(f"\n{'='*60}")
    logger.info(f"æµ‹è¯•å…¬å¸: {company_name}")
    logger.info(f"{'='*60}")
    
    jobs = await scraper.scrape(company)
    
    if jobs:
        logger.info(f"âœ… æˆåŠŸæŠ“å– {len(jobs)} ä¸ªèŒä½\n")
        
        # æ˜¾ç¤ºå‰3ä¸ªèŒä½
        for i, job in enumerate(jobs[:3], 1):
            logger.info(f"{i}. {job['title']}")
            logger.info(f"   åœ°ç‚¹: {job['location']}")
            logger.info(f"   ç±»å‹: {job['job_type']} / {job['remote_type']}")
            if job['skills']:
                logger.info(f"   æŠ€èƒ½: {', '.join(job['skills'][:5])}")
            logger.info(f"   é“¾æ¥: {job['source_url']}\n")
        
        return jobs
    else:
        logger.warning(f"âš ï¸  æœªæ‰¾åˆ°èŒä½\n")
        return []


async def save_jobs_to_db(jobs: list):
    """ä¿å­˜èŒä½åˆ°æ•°æ®åº“"""
    if not jobs:
        return
    
    db = get_db()
    
    saved_count = 0
    skipped_count = 0
    
    for job in jobs:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = db.jobs.find_one({'job_id': job['job_id']})
        
        if not existing:
            db.jobs.insert_one(job)
            saved_count += 1
        else:
            # æ›´æ–°scraped_at
            db.jobs.update_one(
                {'job_id': job['job_id']},
                {'$set': {'scraped_at': job['scraped_at']}}
            )
            skipped_count += 1
    
    logger.info(f"ğŸ’¾ æ•°æ®åº“: æ–°å¢ {saved_count} ä¸ª, å·²å­˜åœ¨ {skipped_count} ä¸ª")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯• Greenhouse é‡‡é›†å™¨\n")
    
    scraper = GreenhouseScraper()
    
    # æµ‹è¯•å‡ å®¶å…¬å¸
    test_companies = [
        ('Airbnb', 'airbnb.com'),
        ('Stripe', 'stripe.com'),
        ('Netflix', 'netflix.com'),
    ]
    
    all_jobs = []
    
    for company_name, domain in test_companies:
        jobs = await test_single_company(scraper, company_name, domain)
        all_jobs.extend(jobs)
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(2)
    
    logger.info(f"\n{'='*60}")
    logger.info(f"æµ‹è¯•æ€»ç»“")
    logger.info(f"{'='*60}")
    logger.info(f"æµ‹è¯•å…¬å¸æ•°: {len(test_companies)}")
    logger.info(f"æ€»å…±æŠ“å–: {len(all_jobs)} ä¸ªèŒä½")
    
    if all_jobs:
        # ä¿å­˜åˆ°æ•°æ®åº“
        logger.info(f"\næ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“...")
        await save_jobs_to_db(all_jobs)
        
        # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
        db = get_db()
        total_in_db = db.jobs.count_documents({})
        logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­èŒä½æ€»æ•°: {total_in_db}")
        
        # æŒ‰å…¬å¸åˆ†ç»„ç»Ÿè®¡
        pipeline = [
            {'$group': {'_id': '$company', 'count': {'$sum': 1}}},
            {'$sort': {'count': -1}}
        ]
        stats = list(db.jobs.aggregate(pipeline))
        
        logger.info(f"\næŒ‰å…¬å¸ç»Ÿè®¡:")
        for stat in stats:
            logger.info(f"  - {stat['_id']}: {stat['count']} ä¸ªèŒä½")
    
    logger.info(f"\nâœ… æµ‹è¯•å®Œæˆ!")
    
    close_db()


if __name__ == '__main__':
    asyncio.run(main())
